import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
from urllib.parse import urljoin, quote
from dataclasses import dataclass
from typing import List, Optional

# Simple logging replacement
class SimpleLogger:
    def info(self, msg):
        print(f"INFO: {msg}")
    
    def error(self, msg):
        print(f"ERROR: {msg}")
    
    def warning(self, msg):
        print(f"WARNING: {msg}")

logger = SimpleLogger()

@dataclass
class JobListing:
    title: str
    company: str
    location: str
    salary: str
    source: str
    url: str

class JobScraper:
    def __init__(self, delay=3):
        self.delay = delay
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
    
    def get_page(self, url: str):
        try:
            time.sleep(self.delay)
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'html.parser')
        except requests.RequestException as e:
            logger.error(f"Error fetching {url}: {e}")
            return None
    
    def extract_salary(self, text: str):
        if not text:
            return "Not specified"
        
        text = text.replace('\n', ' ').replace('\t', ' ').strip()
        
        salary_patterns = [
            r'\$[\d,]+(?:\.\d{2})?\s*(?:-|to)\s*\$[\d,]+(?:\.\d{2})?(?:\s*(?:per\s*year|annually|/year|/yr))?',
            r'\$[\d,]+(?:\.\d{2})?(?:\s*(?:per\s*year|annually|/year|/yr))?',
            r'[\d,]+k\s*(?:-|to)\s*[\d,]+k(?:\s*(?:per\s*year|annually|/year|/yr))?',
            r'[\d,]+k(?:\s*(?:per\s*year|annually|/year|/yr))?',
            r'[\d,]+\s*(?:-|to)\s*[\d,]+\s*(?:per\s*year|annually|/year|/yr)',
            r'Up to \$[\d,]+(?:\.\d{2})?',
            r'From \$[\d,]+(?:\.\d{2})?',
        ]
        
        for pattern in salary_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group().strip()
        
        return "Not specified"
    
    def scrape_indeed(self, job_title: str, location: str):
        jobs = []
        url = f"https://www.indeed.com/jobs?q={quote(job_title)}&l={quote(location)}&sort=date"
        logger.info(f"Scraping Indeed: {url}")
        
        soup = self.get_page(url)
        if not soup:
            return jobs
        
        job_cards = soup.find_all('div', attrs={'data-jk': True}) or soup.find_all('div', class_=re.compile('job_seen_beacon'))
        
        if not job_cards:
            job_cards = soup.find_all('div', class_=re.compile('slider_container|jobsearch-SerpJobCard'))
        
        for card in job_cards[:5]:
            try:
                title_elem = (card.find('h2', class_=re.compile('jobTitle')) or
                             card.find('a', attrs={'data-jk': True}) or
                             card.find('h2') or
                             card.find('a', href=re.compile('/viewjob')))
                
                company_elem = (card.find('span', class_=re.compile('companyName')) or
                               card.find('a', class_=re.compile('companyName')))
                
                location_elem = (card.find('div', attrs={'data-testid': 'job-location'}) or
                                card.find('div', class_=re.compile('companyLocation')))
                
                salary_elem = (card.find('span', class_=re.compile('salary')) or
                              card.find('div', class_=re.compile('salary')))
                
                job_url = ""
                if title_elem:
                    if title_elem.name == 'a' and title_elem.get('href'):
                        job_url = urljoin("https://www.indeed.com", title_elem['href'])
                    else:
                        nested_link = title_elem.find('a')
                        if nested_link and nested_link.get('href'):
                            job_url = urljoin("https://www.indeed.com", nested_link['href'])
                
                title = title_elem.get_text(strip=True) if title_elem else "N/A"
                company = company_elem.get_text(strip=True) if company_elem else "N/A"
                location_text = location_elem.get_text(strip=True) if location_elem else "N/A"
                salary = self.extract_salary(salary_elem.get_text(strip=True) if salary_elem else "")
                
                if title != "N/A" and company != "N/A" and len(title) > 3:
                    jobs.append(JobListing(
                        title=title,
                        company=company,
                        location=location_text,
                        salary=salary,
                        source="Indeed",
                        url=job_url
                    ))
                    
            except Exception as e:
                logger.warning(f"Error parsing Indeed job card: {e}")
                continue
        
        logger.info(f"Found {len(jobs)} jobs on Indeed")
        return jobs
    
    def scrape_linkedin(self, job_title: str, location: str):
        jobs = []
        logger.info("LinkedIn scraping requires authentication - using sample data")
        
        sample_jobs = [
            JobListing(
                title="Senior Graphic Designer",
                company="Creative Agency LA",
                location="Los Angeles, CA",
                salary="$65,000 - $80,000",
                source="LinkedIn",
                url="https://linkedin.com/jobs/sample1"
            ),
            JobListing(
                title="Graphic Designer",
                company="Marketing Firm",
                location="Los Angeles, CA",
                salary="$50,000 - $65,000",
                source="LinkedIn",
                url="https://linkedin.com/jobs/sample2"
            )
        ]
        
        logger.info(f"Found {len(sample_jobs)} sample jobs on LinkedIn")
        return sample_jobs

def save_to_csv(jobs, filename="graphic_designer_jobs_la.csv"):
    if not jobs:
        print("WARNING: No jobs to save")
        return
    
    data = []
    for job in jobs:
        data.append({
            'Title': job.title,
            'Company': job.company,
            'Location': job.location,
            'Salary': job.salary,
            'Source': job.source,
            'URL': job.url
        })
    
    df = pd.DataFrame(data)
    df.to_csv(filename, index=False)
    print(f"INFO: Saved {len(jobs)} jobs to {filename}")

def analyze_salaries(jobs):
    salary_data = []
    
    for job in jobs:
        if job.salary and job.salary != "Not specified":
            salary_text = job.salary.replace('$', '').replace(',', '').replace('k', '000').replace('K', '000')
            numbers = re.findall(r'\d+(?:\.\d+)?', salary_text)
            
            if numbers:
                try:
                    numeric_values = [float(num) for num in numbers]
                    
                    if len(numeric_values) >= 2:
                        avg_sal = sum(numeric_values[:2]) / 2
                        salary_data.append(avg_sal)
                    elif len(numeric_values) == 1:
                        salary_data.append(numeric_values[0])
                        
                except (ValueError, IndexError):
                    logger.warning(f"Could not parse salary: {job.salary}")
                    continue
    
    if salary_data:
        filtered_salaries = [s for s in salary_data if 20000 <= s <= 200000]
        
        if filtered_salaries:
            print(f"\n--- Salary Analysis for Graphic Designers in LA ---")
            print(f"Jobs analyzed: {len(jobs)}")
            print(f"Jobs with salary data: {len(salary_data)}")
            print(f"Jobs after filtering outliers: {len(filtered_salaries)}")
            print(f"Average Salary: ${sum(filtered_salaries) / len(filtered_salaries):,.2f}")
            print(f"Median Salary: ${sorted(filtered_salaries)[len(filtered_salaries)//2]:,.2f}")
            print(f"Min Salary: ${min(filtered_salaries):,.2f}")
            print(f"Max Salary: ${max(filtered_salaries):,.2f}")
        else:
            print("No valid salary data after filtering")
    else:
        print("No salary data available for analysis")

def main():
    print("Starting job scraping for Graphic Designer positions in Los Angeles...")
    print("Note: This script respects robots.txt and implements rate limiting.")
    print("Some sites may have anti-scraping measures that limit results.\n")
    
    scraper = JobScraper(delay=2)
    job_title = "graphic designer"
    location = "Los Angeles, CA"
    
    all_jobs = []
    
    try:
        print("Scraping Indeed...")
        indeed_jobs = scraper.scrape_indeed(job_title, location)
        all_jobs.extend(indeed_jobs)
        
        print("Adding LinkedIn sample data...")
        linkedin_jobs = scraper.scrape_linkedin(job_title, location)
        all_jobs.extend(linkedin_jobs)
        
    except KeyboardInterrupt:
        print("\nScraping interrupted by user")
    except Exception as e:
        print(f"ERROR: Unexpected error: {e}")
    
    # Remove duplicates
    seen = set()
    unique_jobs = []
    for job in all_jobs:
        job_key = (job.title.lower(), job.company.lower())
        if job_key not in seen:
            seen.add(job_key)
            unique_jobs.append(job)
    
    all_jobs = unique_jobs
    
    # Display results
    if all_jobs:
        save_to_csv(all_jobs)
        
        print(f"\nFound {len(all_jobs)} unique job listings:")
        print("-" * 60)
        for job in all_jobs:
            print(f"{job.title} at {job.company}")
            print(f"  Salary: {job.salary} | Source: {job.source}")
            if job.url:
                print(f"  URL: {job.url}")
            print()
        
        analyze_salaries(all_jobs)
        
    else:
        print("No jobs found. This might be due to:")
        print("- Website structure changes requiring selector updates")
        print("- Rate limiting or IP blocking")
        print("- Network connectivity issues")
        print("- Sites requiring authentication or CAPTCHA")

if __name__ == "__main__":
    main()
